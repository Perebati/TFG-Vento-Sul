{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 26 Complete [00h 01m 07s]\n",
      "val_loss: 8.176505088806152\n",
      "\n",
      "Best val_loss So Far: 4.493888854980469\n",
      "Total elapsed time: 00h 15m 30s\n",
      "\n",
      "Melhores hiperparâmetros encontrados:\n",
      "- Filtros: 208\n",
      "- Tamanho do Kernel: 5\n",
      "- Tamanho do Pooling: 3\n",
      "- Unidades LSTM: 192\n",
      "- Taxa de Aprendizado: 0.0001\n",
      "- Taxa de Dropout: 0.2\n",
      "- Otimizador: rmsprop\n",
      "- Número de Cabeças de Atenção: 4\n",
      "\n",
      "Epoch 1/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - loss: 7.7189 - mae: 0.5054 - rmse: 0.6849 - val_loss: 7.1361 - val_mae: 0.2069 - val_rmse: 0.2527\n",
      "Epoch 2/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 3.3214 - mae: 0.1766 - rmse: 0.2197 - val_loss: 5.9568 - val_mae: 0.1671 - val_rmse: 0.2008\n",
      "Epoch 3/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 2.9239 - mae: 0.1419 - rmse: 0.1755 - val_loss: 10.3118 - val_mae: 0.2241 - val_rmse: 0.2760\n",
      "Epoch 4/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 2.5406 - mae: 0.1191 - rmse: 0.1502 - val_loss: 6.9132 - val_mae: 0.1602 - val_rmse: 0.1952\n",
      "Epoch 5/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 2.3928 - mae: 0.1096 - rmse: 0.1387 - val_loss: 6.2564 - val_mae: 0.1536 - val_rmse: 0.1850\n",
      "Epoch 6/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 2.2518 - mae: 0.1022 - rmse: 0.1306 - val_loss: 7.0116 - val_mae: 0.1544 - val_rmse: 0.1947\n",
      "Epoch 7/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 2.1647 - mae: 0.1013 - rmse: 0.1282 - val_loss: 8.6854 - val_mae: 0.1854 - val_rmse: 0.2332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo salvo em: best_bidirectional_lstm_model.h5\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. None expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 307\u001b[0m\n\u001b[1;32m    304\u001b[0m y_test_class \u001b[38;5;241m=\u001b[39m (y_test_inv \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m6.0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Avaliação das previsões\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m mae \u001b[38;5;241m=\u001b[39m \u001b[43mmean_absolute_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_inv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_inv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(y_test_inv, y_pred_inv))\n\u001b[1;32m    309\u001b[0m mape \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs((y_test_inv \u001b[38;5;241m-\u001b[39m y_pred_inv) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(y_test_inv, \u001b[38;5;241m1e-6\u001b[39m))) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Evitar divisão por zero\u001b[39;00m\n",
      "File \u001b[0;32m~/Projetos/TFG-Vento-Sul/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/Projetos/TFG-Vento-Sul/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py:216\u001b[0m, in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    153\u001b[0m     {\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m ):\n\u001b[1;32m    164\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_absolute_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    np.float64(0.85...)\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    220\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(np\u001b[38;5;241m.\u001b[39mabs(y_pred \u001b[38;5;241m-\u001b[39m y_true), weights\u001b[38;5;241m=\u001b[39msample_weight, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Projetos/TFG-Vento-Sul/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py:113\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[1;32m    111\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    112\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    116\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mreshape(y_true, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/Projetos/TFG-Vento-Sul/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:1058\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1056\u001b[0m     )\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m   1064\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1065\u001b[0m         array,\n\u001b[1;32m   1066\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1067\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1068\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1069\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. None expected <= 2."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, mean_absolute_error, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D, MaxPooling1D, LSTM, Dropout, LayerNormalization, Dense, TimeDistributed,\n",
    "    RepeatVector, MultiHeadAttention, Concatenate, Bidirectional\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras_tuner import Hyperband\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# %%\n",
    "# Carregar dados\n",
    "data = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Converter 'id' em datetime e definir como índice\n",
    "data['timestamp'] = pd.to_datetime(data['id'], errors='coerce')\n",
    "data.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Selecionar variáveis relevantes\n",
    "variables = data[['ws100', 'humid', 'wdisp100', 'hour', 'wdir100']]\n",
    "\n",
    "# Padronização dos dados usando MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "variables_scaled = scaler.fit_transform(variables)\n",
    "\n",
    "# Parâmetros para sequências de entrada e saída\n",
    "sequence_length = 36   # Janela de aprendizado de 36 passos de tempo\n",
    "step_ahead = 6         # Previsão para o sexto passo (1 hora de antecedência)\n",
    "split_ratio = 0.80     # 80% para treinamento e 20% para teste\n",
    "\n",
    "# Divisão dos dados em treinamento e teste\n",
    "split_index = int(len(variables_scaled) * split_ratio)\n",
    "train_data = variables_scaled[:split_index]\n",
    "test_data = variables_scaled[split_index:]\n",
    "\n",
    "# Função para preparar sequências de dados para previsão de um passo específico\n",
    "def create_sequences_single_ahead(data, seq_length, step_ahead=6):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length - step_ahead + 1):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i + seq_length + step_ahead -1, 0])  # ws100 é a primeira coluna\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Criar sequências para treinamento e teste\n",
    "X_train, y_train = create_sequences_single_ahead(train_data, sequence_length, step_ahead)\n",
    "X_test, y_test = create_sequences_single_ahead(test_data, sequence_length, step_ahead)\n",
    "\n",
    "# Reshape de y_train e y_test para serem compatíveis com a saída do modelo\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "# Valores mínimos e máximos de 'ws100' para inversão da escala\n",
    "ws100_min = scaler.data_min_[0]\n",
    "ws100_max = scaler.data_max_[0]\n",
    "\n",
    "# Número de características\n",
    "num_features = X_train.shape[2]\n",
    "\n",
    "# Verificação das formas dos dados\n",
    "print(f\"X_train shape: {X_train.shape}\")  # Esperado: (num_samples, 36, 5)\n",
    "print(f\"y_train shape: {y_train.shape}\")  # Esperado: (num_samples, 1)\n",
    "print(f\"X_test shape: {X_test.shape}\")    # Esperado: (num_samples, 36, 5)\n",
    "print(f\"y_test shape: {y_test.shape}\")    # Esperado: (num_samples, 1)\")\n",
    "\n",
    "# %%\n",
    "# Função de Data Augmentation para amostras com y < 6 m/s\n",
    "def augment_data(X, y, num_augmented_per_sample=5):\n",
    "    X_augmented = []\n",
    "    y_augmented = []\n",
    "    for i in range(len(y)):\n",
    "        # Inversão da normalização de y\n",
    "        y_inv = y[i] * (ws100_max - ws100_min) + ws100_min\n",
    "        if y_inv < 6.0:\n",
    "            for _ in range(num_augmented_per_sample):\n",
    "                noise = np.random.normal(0, 0.01, X[i].shape)\n",
    "                X_augmented.append(X[i] + noise)\n",
    "                y_augmented.append(y[i])\n",
    "    if X_augmented:\n",
    "        X_augmented = np.array(X_augmented)\n",
    "        y_augmented = np.array(y_augmented)\n",
    "        X = np.concatenate((X, X_augmented), axis=0)\n",
    "        y = np.concatenate((y, y_augmented), axis=0)\n",
    "    return X, y\n",
    "\n",
    "# Aplicar data augmentation nos dados de treinamento\n",
    "X_train, y_train = augment_data(X_train, y_train)\n",
    "\n",
    "print(f\"After augmentation, X_train shape: {X_train.shape}\")\n",
    "print(f\"After augmentation, y_train shape: {y_train.shape}\")\n",
    "\n",
    "# %%\n",
    "# Função de perda personalizada sensível a outliers\n",
    "def get_custom_loss(ws100_min, ws100_max):\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        # Inversão da normalização para obter valores originais\n",
    "        y_true_inv = y_true * (ws100_max - ws100_min) + ws100_min\n",
    "        y_pred_inv = y_pred * (ws100_max - ws100_min) + ws100_min\n",
    "\n",
    "        # Calcula o erro absoluto\n",
    "        error = K.abs(y_true_inv - y_pred_inv)\n",
    "\n",
    "        # Cria um tensor booleano indicando onde y_true_inv < 6 m/s\n",
    "        condition = K.less(y_true_inv, 6.0)\n",
    "\n",
    "        # Define um peso maior para velocidades abaixo de 6 m/s\n",
    "        greater_weight = tf.cast(5.0, y_true.dtype)  # Peso maior para erros quando y_true_inv < 6 m/s\n",
    "        lesser_weight = tf.cast(1.0, y_true.dtype)   # Peso normal para outros casos\n",
    "\n",
    "        # Aplica os pesos garantindo que tenham a mesma forma que 'error'\n",
    "        weights = K.switch(condition, greater_weight * K.ones_like(error), lesser_weight * K.ones_like(error))\n",
    "        # Alternativamente, você pode usar:\n",
    "        # weights = K.cast(condition, y_true.dtype) * (greater_weight - lesser_weight) + lesser_weight\n",
    "\n",
    "        weighted_error = weights * error\n",
    "\n",
    "        # Retorna a média do erro ponderado\n",
    "        return K.mean(weighted_error)\n",
    "    \n",
    "    return custom_loss\n",
    "\n",
    "# %%\n",
    "# Função para construir o modelo com atenção e LSTM bidirecional\n",
    "def build_model_cnn(hp):\n",
    "    # Hiperparâmetros com espaços de busca reduzidos\n",
    "    filters = hp.Int('filters', min_value=16, max_value=256, step=16)\n",
    "    kernel_size = hp.Choice('kernel_size', values=[3, 5, 7])\n",
    "    pool_size = hp.Choice('pool_size', values=[2, 3])\n",
    "    units = hp.Int('units', min_value=16, max_value=256, step=16)\n",
    "    learning_rate = hp.Choice('learning_rate', [1e-4, 1e-3, 1e-2])\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
    "    num_heads = hp.Int('num_heads', min_value=2, max_value=6, step=2)\n",
    "    \n",
    "    # Seleção do Otimizador\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    \n",
    "    # Definir a função de perda personalizada com ws100_min e ws100_max\n",
    "    custom_loss = get_custom_loss(ws100_min, ws100_max)\n",
    "    \n",
    "    # Entrada do Encoder\n",
    "    encoder_inputs = tf.keras.Input(shape=(sequence_length, num_features), name='encoder_input')\n",
    "    x = encoder_inputs\n",
    "\n",
    "    # Camada CNN\n",
    "    x = Conv1D(\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        kernel_regularizer=l2(1e-4),\n",
    "        name='conv1d_layer'\n",
    "    )(x)\n",
    "\n",
    "    # Camada de Pooling\n",
    "    x = MaxPooling1D(pool_size=pool_size, padding='same', name='maxpool_layer')(x)\n",
    "\n",
    "    # Camada LSTM Bidirecional\n",
    "    x = Bidirectional(LSTM(\n",
    "        units=units,\n",
    "        return_sequences=True,\n",
    "        activation='tanh',\n",
    "        dropout=dropout_rate,\n",
    "        kernel_regularizer=l2(1e-3),\n",
    "        name='bidirectional_lstm_layer'\n",
    "    ))(x)\n",
    "    x = LayerNormalization(name='lstm_norm')(x)\n",
    "\n",
    "    # Projetar a saída da LSTM bidirecional de volta para 'units' dimensões\n",
    "    encoder_proj = Dense(units, activation='relu', name='encoder_projection')(x)\n",
    "\n",
    "    # Encoder State (última saída de LSTM)\n",
    "    encoder_state = encoder_proj[:, -1, :]  # Forma: (batch_size, units)\n",
    "\n",
    "    # Decoder Input\n",
    "    decoder_inputs = RepeatVector(1, name='repeat_vector')(encoder_state)  # Forma: (batch_size, 1, units)\n",
    "    decoder_outputs = decoder_inputs\n",
    "\n",
    "    # Camada LSTM no Decoder\n",
    "    decoder_outputs = LSTM(\n",
    "        units=units,\n",
    "        return_sequences=True,\n",
    "        activation='tanh',\n",
    "        dropout=dropout_rate,\n",
    "        kernel_regularizer=l2(1e-3),\n",
    "        name='decoder_lstm_layer'\n",
    "    )(decoder_outputs)\n",
    "    decoder_outputs = LayerNormalization(name='decoder_lstm_norm')(decoder_outputs)\n",
    "\n",
    "    # Atenção MultiCabeça\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=units,  # key_dim agora corresponde a 'units'\n",
    "        name='multi_head_attention'\n",
    "    )(\n",
    "        query=decoder_outputs,  # (batch_size, 1, units)\n",
    "        key=encoder_proj,        # (batch_size, sequence_length, units)\n",
    "        value=encoder_proj       # (batch_size, sequence_length, units)\n",
    "    )\n",
    "    \n",
    "    # Aplicar Dropout na saída da atenção\n",
    "    attention_output = Dropout(dropout_rate, name='attention_dropout')(attention_output)\n",
    "    \n",
    "    # Concatenar a saída da atenção com o decoder_outputs\n",
    "    decoder_concat_input = Concatenate(axis=-1, name='concat_attention')([decoder_outputs, attention_output])  # (batch_size, 1, units + units)\n",
    "\n",
    "    # Camada de Saída\n",
    "    outputs = TimeDistributed(Dense(1, activation='linear'), name='output_layer')(decoder_concat_input)  # (batch_size, 1, 1)\n",
    "\n",
    "    # Definição do Modelo\n",
    "    model = Model(inputs=encoder_inputs, outputs=outputs, name='Wind_Speed_Predictor_CNN_BiLSTM_Attention')\n",
    "\n",
    "    # Compilação do Modelo com Função de Loss Personalizada\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=custom_loss,\n",
    "        metrics=[\n",
    "            tf.keras.metrics.RootMeanSquaredError(name='rmse'),\n",
    "            tf.keras.metrics.MeanAbsoluteError(name='mae')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# %%\n",
    "# Definir o tuner usando Hyperband\n",
    "tuner = Hyperband(\n",
    "    build_model_cnn,\n",
    "    objective='val_loss',  # Otimizar a perda de validação\n",
    "    max_epochs=20,\n",
    "    factor=3,\n",
    "    directory='tuner_dir',\n",
    "    project_name='bidirectional_lstm_tuning_updated'\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Callback para parada antecipada\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# %%\n",
    "# Iniciar a busca de hiperparâmetros\n",
    "tuner.search(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Obter os melhores hiperparâmetros\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "Melhores hiperparâmetros encontrados:\n",
    "- Filtros: {best_hps.get('filters')}\n",
    "- Tamanho do Kernel: {best_hps.get('kernel_size')}\n",
    "- Tamanho do Pooling: {best_hps.get('pool_size')}\n",
    "- Unidades LSTM: {best_hps.get('units')}\n",
    "- Taxa de Aprendizado: {best_hps.get('learning_rate')}\n",
    "- Taxa de Dropout: {best_hps.get('dropout_rate')}\n",
    "- Otimizador: {best_hps.get('optimizer')}\n",
    "- Número de Cabeças de Atenção: {best_hps.get('num_heads')}\n",
    "\"\"\")\n",
    "\n",
    "# %%\n",
    "# Construir o melhor modelo com os hiperparâmetros encontrados\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Treinar o melhor modelo\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Salvar o modelo final\n",
    "model_save_path_final = 'best_bidirectional_lstm_model.h5.keras'\n",
    "model.save(model_save_path_final)\n",
    "print(f\"Modelo salvo em: {model_save_path_final}\")\n",
    "\n",
    "# %%\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inversão da padronização\n",
    "y_pred_inv = y_pred * (ws100_max - ws100_min) + ws100_min\n",
    "y_test_inv = y_test * (ws100_max - ws100_min) + ws100_min\n",
    "\n",
    "# Converter para classes: 1 se y < 6 m/s, 0 caso contrário\n",
    "y_pred_class = (y_pred_inv < 6.0).astype(int)\n",
    "y_test_class = (y_test_inv < 6.0).astype(int)\n",
    "\n",
    "# Avaliação das previsões\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "mape = np.mean(np.abs((y_test_inv - y_pred_inv) / np.maximum(y_test_inv, 1e-6))) * 100  # Evitar divisão por zero\n",
    "\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\\n\")\n",
    "\n",
    "# Relatório de Classificação\n",
    "print(\"Relatório de Classificação:\")\n",
    "print(classification_report(y_test_class, y_pred_class))\n",
    "\n",
    "# Plotagem das previsões vs valores reais focando em y < 6 m/s\n",
    "indices_below_6 = np.where(y_test_inv < 6.0)[0]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(indices_below_6, y_test_inv[indices_below_6], label='Valor Real')\n",
    "plt.plot(indices_below_6, y_pred_inv[indices_below_6], label='Previsão')\n",
    "plt.legend()\n",
    "plt.title('Comparação entre Valores Reais e Previstos para y < 6 m/s')\n",
    "plt.xlabel('Amostras')\n",
    "plt.ylabel('Velocidade do Vento a 100 metros')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Valores únicos em y_test_class: [0 1]\n",
      "Valores únicos em y_pred_class: [0 1]\n",
      "MAE: 1.4868\n",
      "RMSE: 1.9263\n",
      "MAPE: 18.69%\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92      1350\n",
      "           1       0.27      0.41      0.33       122\n",
      "\n",
      "    accuracy                           0.86      1472\n",
      "   macro avg       0.61      0.66      0.62      1472\n",
      "weighted avg       0.89      0.86      0.87      1472\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fazer previsões no conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inversão da padronização\n",
    "y_pred_inv = y_pred * (ws100_max - ws100_min) + ws100_min  # Forma: (batch_size, 1, 1)\n",
    "y_test_inv = y_test * (ws100_max - ws100_min) + ws100_min  # Forma: (batch_size, 1)\n",
    "\n",
    "# Achatar os arrays para 1D\n",
    "y_pred_inv = y_pred_inv.reshape(-1)\n",
    "y_test_inv = y_test_inv.reshape(-1)\n",
    "\n",
    "# Converter para classes: 1 se y < 6 m/s, 0 caso contrário\n",
    "y_pred_class = (y_pred_inv < 6.0).astype(int)\n",
    "y_test_class = (y_test_inv < 6.0).astype(int)\n",
    "\n",
    "# Verificar valores únicos\n",
    "print(\"Valores únicos em y_test_class:\", np.unique(y_test_class))\n",
    "print(\"Valores únicos em y_pred_class:\", np.unique(y_pred_class))\n",
    "\n",
    "# Avaliação das previsões\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "mape = np.mean(np.abs((y_test_inv - y_pred_inv) / np.maximum(y_test_inv, 1e-6))) * 100  # Evitar divisão por zero\n",
    "\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\\n\")\n",
    "\n",
    "# Relatório de Classificação\n",
    "print(\"Relatório de Classificação:\")\n",
    "print(classification_report(y_test_class, y_pred_class))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
