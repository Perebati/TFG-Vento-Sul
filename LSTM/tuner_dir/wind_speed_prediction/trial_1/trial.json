{"trial_id": "1", "hyperparameters": {"space": [{"class_name": "Int", "config": {"name": "units", "default": null, "conditions": [], "min_value": 64, "max_value": 256, "step": 64, "sampling": "linear"}}, {"class_name": "Choice", "config": {"name": "learning_rate", "default": 0.001, "conditions": [], "values": [0.001, 0.0001], "ordered": true}}], "values": {"units": 64, "learning_rate": 0.0001}}, "metrics": {"metrics": {}}, "score": null, "best_step": 0, "status": "FAILED", "message": "Traceback (most recent call last):\n  File \"/home/lucas/Projetos/TFG-Vento-Sul/.venv/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/lucas/Projetos/TFG-Vento-Sul/.venv/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/lucas/Projetos/TFG-Vento-Sul/.venv/lib/python3.10/site-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/home/lucas/Projetos/TFG-Vento-Sul/.venv/lib/python3.10/site-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n  File \"/home/lucas/Projetos/TFG-Vento-Sul/.venv/lib/python3.10/site-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n  File \"/home/lucas/Projetos/TFG-Vento-Sul/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/home/lucas/Projetos/TFG-Vento-Sul/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\nValueError: Exception encountered when calling Attention.call().\n\n\u001b[1mDimension must be 2 but is 3 for '{{node functional_1/attention_1/transpose}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](functional_1/lstm_1/strided_slice_3, functional_1/attention_1/transpose/perm)' with input shapes: [?,64], [3].\u001b[0m\n\nArguments received by Attention.call():\n  \u2022 inputs=['tf.Tensor(shape=(None, 6, 64), dtype=float32)', 'tf.Tensor(shape=(None, 64), dtype=float32)']\n  \u2022 mask=['None', 'None']\n  \u2022 training=True\n  \u2022 return_attention_scores=False\n  \u2022 use_causal_mask=False\n"}